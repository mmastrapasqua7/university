# Differential privacy

Vuole prevenire che un avversario sia capace di dire se una persona è presente o assente in un dataset

Definisce una **proprietà del meccanismo di rilascio**

Richiede informalmente che la distribuzione della probabilità sul risultato di un analisi è essenzialmente la stessa sia che ci sia o non ci sia un determinato individuo nel database di partenza

Applicabile in 2 scenari

- **scenario interattivo**, ovvero a runtime (statistical dbms)

- **scenario non interattivo**, rilascio di tabelle pre-computate
  
  E' proprio in questo scenario che è nata la differential privacy, poi successivamente applicata agli scenari interattivi (statistical data)

Altri 2 scenari riguardo il **TRUST ASSUMPTION**

- global differential privacy (rilascio all'intera comunità scientifica, al mondo)

- local differential privacy (rilascio a un ospedale/università per ricerca)

Meccanismo: **aggiungere del rumore random nei dati**, dunque la correttezza dei dati non è preservata, è perturbativa

Il fattore importante è $\epsilon$, che diventerà il budget della query: limite sulle query. $\epsilon \in (0, 1)$ è la misura del rumore. Più è basso $\epsilon$ più sono protetto

## Variazioni e applicazioni

## Limiti

La differential privacy dice solamente che la presenza o meno di una persona non influenza i dati, ma la generazione dei dati stessi può essere influenzata

## k-anonymity vs differential privacy

K-anon

- +più elegante per requisiti nel real world

- -non protegge completamente

differential privacy

- +garanzie sulla protezione

- -non facile da capire e non garantisce nemmeno completa protezione

---

# Distribuzione di dati sensibili

Le tuple individuali non sono sensibili, ma una collezione di tuple possono leakare delle informazioni sensibili che non sono esplicitamente riportate

- esempio: record medici di soldati
  
  non sono sensibili di per se, ma se
  
  - distribuzione età in una location -> tipo di location
  
  esempio: soldati giovani si trovano nei campus di training, se sono più vecchi sono nell'HQ
  
  Posso comparare le statistiche pubbliche con quelle osservate

Il problema è quando i dati sfuggono via dal **trusted environment** verso l'**external environment**

## Contrattaccare i canali di inferenza

- **QUAL E'** l'informazione sensibile
  
  - distribuzione dei valori per identificare outlier

- **QUANDO** rinforzare l'esposizione
  
  - i valori rilasciati non rispecchiano quelli reali

- **COME** rinforzare l'esposizione

---

# Privacy e dati del genoma

Sono un'opportunità per la medicina ma un grosso problema di privacy

- identificano univocamente il rispondente

- contengono altri dati riguardo l'eredità di etnia, esposizione a malattie, tratti

- non espongono solo te ma anche i tuoi parenti/familiari
