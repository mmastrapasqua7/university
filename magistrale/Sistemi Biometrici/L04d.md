# Rappresentazione, compressione, non unicità

- sensore impiegato

- livello di analisi

- caratteristiche estratte

## Immagini delle impronte

### Compressione

Esempio: l'FBI usa 500 DPi e 8 bit per pixel per scansionare impronte offline. 10 impronte occupano 10 MB!

Serve fare compressione. Usando il sistema **lossless** (senza perdita) si ottiene una compressione del 50%, ovvero fattore 2:1. 

Usando **JPEG** (lossy) si comprime di più, fino a 13:1. Produce blocchettizzazione. Il JPEG fa schifo e **non va usato** per la compressione di impronte

Usando **WSQ** abbiamo ancora 13:1 ma è migliore per le impronte. è anche questo lossy ma funziona bene e ci va bene

### Formato interscambio

### Unicità delle impronte (minuzie)

è possibile stimare, come fece Galton, la possibilità di stesse minuizie tra 2 persone diverse

- **La zona in comune è datta OVERLAP**

- **Area di tolleranza**, se è un area di overlap X, allora la tolleranza sarà 1/X, ovvero una frazione dell'overlap. Area di tolleranza **intorno alle minuzie**

Data un impronta con $n$ minuzie, $n$ piccolo. E' possibile calcolare la probabilità di condividere $q$ minuzie con un altro template contenete $m$ minuzie

$p(M,m,n,q)$

$M$ = Area di overlap / Area di tolleranza = A / C

p = Probabilità che qualcuno abbia le stesse minuzie

**Il fattore dominante è $q$, ovvero il numero di minuzie in comune**

Solo a livello 2 (L2), le minuzie riescono a discriminarmi milioni se non miliardi di persone. Con le minuzie riesco a essere certo. $q$ discrimina, ovvero numero di minuzie in comune

Abbiamo in realtà molti altri parametri per l'impronta, come ridge count.

## Spoofing
